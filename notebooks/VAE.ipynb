{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f224b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e90168c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.96544691  1.89090018  1.63103472  1.55475405  1.57971671\n",
      "    1.47559902  1.54613365  1.20877907  1.2023891   1.16062725\n",
      "    0.90683138  0.36975428  0.21047131  0.50433129  0.66391452\n",
      "    0.93693405  0.9550858   0.78086046  0.50445886  0.2969833\n",
      "   -0.14215755 -0.38787225 -0.33649858 -0.10276504]\n",
      "  [ 2.21877628  2.28797191  2.36326372  2.38857246  2.4282269\n",
      "    2.39361263  2.38429869  2.37805714  2.29728787  2.19367929\n",
      "    2.04724362  1.87403809  1.65201661  1.46909818  1.32787373\n",
      "    1.35979494  1.29665935  1.13896049  0.92962441  0.76941787\n",
      "    0.56851036  0.41031165  0.33490702  0.10594362]\n",
      "  [ 2.48682813  2.38709946  2.33682649  2.19033722  2.13671521\n",
      "    2.07027948  2.07932324  2.02947926  2.0293772   2.11176924\n",
      "    2.16165408  2.26257333  2.34405927  2.2456411   2.09718512\n",
      "    1.81197206  1.53871871  1.49888962  1.31611648  1.13080152\n",
      "    0.84082187  0.61034226  0.50157048  0.39697204]]\n",
      "\n",
      " [[-0.03159875  0.09864043  0.48194915  0.44053271  0.01202576\n",
      "   -0.15674968 -0.16505665 -0.31349278 -0.65934817 -0.70689509\n",
      "   -0.47201103 -0.15114908  0.0656435   0.28625483  0.44859267\n",
      "    0.44536258  0.49206866  0.68285443  0.62790801  0.50989413\n",
      "    0.32582443  0.06433368 -0.2175223  -0.47577886]\n",
      "  [-0.10014223 -0.20314259 -0.35359238 -0.50570309 -0.7186777\n",
      "   -0.88185729 -1.0438174  -1.19565269 -1.21366152 -1.09164682\n",
      "   -0.86705393 -0.72441809 -0.65415423 -0.73824029 -0.95056772\n",
      "   -0.9723072  -0.87788465 -0.845259   -0.80956222 -0.81920373\n",
      "   -0.8254011  -0.84731024 -0.92349776 -0.97004624]\n",
      "  [ 0.37076347  0.57096018  0.49845612  0.30231989  0.15353057\n",
      "    0.07856848 -0.15693006 -0.35308797 -0.4283054  -0.43541634\n",
      "   -0.34918188 -0.23764205 -0.09205282 -0.08923825 -0.08092399\n",
      "    0.04869023  0.13988352  0.23924814  0.2230508   0.08595431\n",
      "    0.02209782 -0.03883996 -0.15192335 -0.12636011]]\n",
      "\n",
      " [[-0.28708622  0.22986908  0.87984962  1.53155969  1.89672901\n",
      "    1.88303185  1.83133203  1.66519372  1.54679269  1.47440666\n",
      "    1.29573565  1.53929559  1.69672257  1.66039928  1.50461335\n",
      "    1.31174672  1.14243377  0.91970233  0.68452754  0.35659833\n",
      "    0.07276062 -0.05965827 -0.30765584 -0.63983163]\n",
      "  [-0.99516341 -0.93822555 -0.76523725 -0.67109026 -0.50888406\n",
      "   -0.54591165 -0.65815434 -0.89425547 -1.21366152 -1.45168256\n",
      "   -1.60719299 -1.66931124 -1.66310397 -1.60198143 -1.56764562\n",
      "   -1.48536967 -1.34055358 -1.29621797 -1.34469657 -1.48112939\n",
      "   -1.69659576 -1.80136823 -1.92154293 -2.08907569]\n",
      "  [-0.02110036  0.25510987  0.69402744  1.11147017  1.33581372\n",
      "    1.53404959  1.60853307  1.54488931  1.362887    1.22025429\n",
      "    1.31052325  1.4004301   1.36106667  1.18833724  1.16370979\n",
      "    1.08857439  0.93255679  0.75243541  0.3141396   0.04241901\n",
      "   -0.27953736 -0.47162811 -0.58758591 -0.73691429]]\n",
      "\n",
      " [[-0.95542023 -1.09050684 -1.20161599 -0.98413621 -0.83999908\n",
      "   -0.76048313 -0.59901591 -0.64446013 -0.61663144 -0.67480402\n",
      "   -0.73982465 -0.58236648 -0.76976571 -0.73756599 -0.73353283\n",
      "   -0.74591561 -0.86397383 -1.06854956 -1.25167486 -1.30032101\n",
      "   -1.23907753 -1.13547077 -1.12787113 -1.03484135]\n",
      "  [-2.13428128 -2.16336383 -2.20599427 -2.24226842 -2.18723318\n",
      "   -2.09966021 -1.85799497 -1.66927688 -1.603767   -1.49668703\n",
      "   -1.46841692 -1.48033261 -1.56701352 -1.60198143 -1.61511315\n",
      "   -1.57865375 -1.47935426 -1.52169746 -1.52307468 -1.52525777\n",
      "   -1.56591656 -1.54117059 -1.53100351 -1.48652137]\n",
      "  [-0.8440144  -0.96881009 -0.94877162 -0.85360909 -0.76178542\n",
      "   -0.61086994 -0.51002268 -0.47423546 -0.4283054  -0.35051015\n",
      "   -0.30662534 -0.23764205 -0.39122448 -0.44167287 -0.48098485\n",
      "   -0.44864567 -0.51290624 -0.64716623 -0.64229287 -0.6976811\n",
      "   -0.66735402 -0.731301   -0.71828467 -0.73691429]]\n",
      "\n",
      " [[-0.93862347 -0.88287795 -0.94120516 -0.85210678 -0.70624595\n",
      "   -0.6156569  -0.7416151  -0.75300318 -0.62553076 -0.58209646\n",
      "   -0.82202487 -0.78801007 -0.78654454 -0.78048063 -0.8879469\n",
      "   -0.64100191 -0.26089178  0.15198845  0.46361854  0.91016648\n",
      "    1.17247174  1.41001102  1.66356485  1.81729384]\n",
      "  [-1.36130844 -1.30576704 -1.38270454 -1.33263896 -1.30609989\n",
      "   -1.21780292 -1.17237175 -1.1095392  -1.08362635 -1.18165576\n",
      "   -1.32964084 -1.3858433  -1.42287784 -1.41003896 -1.33030796\n",
      "   -1.11223333 -1.01668533 -0.93545079 -0.89875128 -0.90746048\n",
      "   -0.91252056 -0.89067651 -0.88010449 -0.84092746]\n",
      "  [-0.72645525 -0.81088493 -0.83142883 -0.85360909 -0.79992359\n",
      "   -0.80238061 -0.78465028 -0.67614794 -0.51161667 -0.47786943\n",
      "   -0.43429496 -0.45317785 -0.4767021  -0.44167287 -0.1698264\n",
      "    0.18432729  0.69941759  0.98570235  1.31611648  1.56615452\n",
      "    1.70263666  1.95198553  2.11352194  2.05419052]]]\n"
     ]
    }
   ],
   "source": [
    "# Ensure data is processed\n",
    "if not os.path.exists(\"../data/X.npy\") or not os.path.exists(\"../data/Y.npy\"):\n",
    "    print(\"Processed data not found. Running VAE_dataprocessing.py...\")\n",
    "    os.system(\"python ../VAE_dataprocessing.py\")\n",
    "\n",
    "# Load processed training data\n",
    "X = np.load(\"../data/X.npy\")\n",
    "Y = np.load(\"../data/Y.npy\")\n",
    "\n",
    "# Train and Validation Sets\n",
    "train_X = X[:39]  # shape: (39, 3, 24)\n",
    "val_X = X[39:]    # shape: (10, 3, 24)\n",
    "train_Y = Y[:39]  # shape: (39, 24)\n",
    "val_Y = Y[39:]    # shape: (10, 24)\n",
    "\n",
    "# Normalize X\n",
    "X_mean = np.mean(train_X, axis=0, keepdims=True)\n",
    "X_std = np.std(train_X, axis=0, keepdims=True)\n",
    "\n",
    "train_X_norm = (train_X - X_mean) / X_std\n",
    "val_X_norm   = (val_X - X_mean) / X_std\n",
    "\n",
    "print(train_X_norm[:5])\n",
    "\n",
    "# Normalize Y\n",
    "Y_mean = np.mean(train_Y, axis=0, keepdims=True)\n",
    "Y_std = np.std(train_Y, axis=0, keepdims=True)\n",
    "\n",
    "# Normalize both training and validation Y using training set statistics\n",
    "train_Y_norm = (train_Y - Y_mean) / Y_std\n",
    "val_Y_norm   = (val_Y - Y_mean) / Y_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1aa836cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tensor = torch.tensor(train_X_norm, dtype=torch.float32)\n",
    "train_Y_tensor = torch.tensor(train_Y_norm, dtype=torch.float32)\n",
    "val_X_tensor   = torch.tensor(val_X_norm, dtype=torch.float32)\n",
    "val_Y_tensor   = torch.tensor(val_Y_norm, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "val_dataset = TensorDataset(val_X_tensor, val_Y_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b27179e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definitions\n",
    "latent_dim = 16\n",
    "x_dim = 3 * 24\n",
    "y_dim = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d46f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(x_dim + y_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        combined = torch.cat([x, y], dim=1)\n",
    "        h = F.relu(self.fc1(combined))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eee368e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, x_dim, latent_dim, y_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(x_dim + latent_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, y_dim)\n",
    "    \n",
    "    def forward(self, x, z):\n",
    "        combined = torch.cat([x, z], dim=1)\n",
    "        h = F.relu(self.fc1(combined))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        y_recon = self.fc3(h)\n",
    "        return y_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24d2b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVAE model: combines encoder, reparameterization, and decoder\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, latent_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.encoder = Encoder(x_dim, y_dim, latent_dim)\n",
    "        self.decoder = Decoder(x_dim, latent_dim, y_dim)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # In training, y is provided to the encoder\n",
    "        mu, logvar = self.encoder(x, y)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        y_recon = self.decoder(x, z)\n",
    "        return y_recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "83f1bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_recon, y, mu, logvar):\n",
    "    recon_loss = F.mse_loss(y_recon, y, reduction='mean')\n",
    "    kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "85f27158",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CVAE(x_dim, y_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0bff9281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 1.0120, Val Loss: 0.8278\n",
      "Epoch 2/100, Train Loss: 0.9829, Val Loss: 0.8157\n",
      "Epoch 3/100, Train Loss: 0.9633, Val Loss: 0.8063\n",
      "Epoch 4/100, Train Loss: 0.9337, Val Loss: 0.8054\n",
      "Epoch 5/100, Train Loss: 0.9140, Val Loss: 0.8275\n",
      "Epoch 6/100, Train Loss: 0.8891, Val Loss: 0.8430\n",
      "Epoch 7/100, Train Loss: 0.8703, Val Loss: 0.8560\n",
      "Epoch 8/100, Train Loss: 0.8557, Val Loss: 0.9243\n",
      "Epoch 9/100, Train Loss: 0.8317, Val Loss: 0.9562\n",
      "Epoch 10/100, Train Loss: 0.8089, Val Loss: 0.9932\n",
      "Epoch 11/100, Train Loss: 0.7757, Val Loss: 1.0405\n",
      "Epoch 12/100, Train Loss: 0.7593, Val Loss: 1.0852\n",
      "Epoch 13/100, Train Loss: 0.7509, Val Loss: 1.0994\n",
      "Epoch 14/100, Train Loss: 0.7236, Val Loss: 1.0999\n",
      "Epoch 15/100, Train Loss: 0.6899, Val Loss: 1.1745\n",
      "Epoch 16/100, Train Loss: 0.6636, Val Loss: 1.1816\n",
      "Epoch 17/100, Train Loss: 0.6487, Val Loss: 1.1837\n",
      "Epoch 18/100, Train Loss: 0.6181, Val Loss: 1.2231\n",
      "Epoch 19/100, Train Loss: 0.5741, Val Loss: 1.2443\n",
      "Epoch 20/100, Train Loss: 0.5749, Val Loss: 1.1475\n",
      "Epoch 21/100, Train Loss: 0.5381, Val Loss: 1.0975\n",
      "Epoch 22/100, Train Loss: 0.4999, Val Loss: 1.2110\n",
      "Epoch 23/100, Train Loss: 0.4750, Val Loss: 1.2055\n",
      "Epoch 24/100, Train Loss: 0.4565, Val Loss: 1.1314\n",
      "Epoch 25/100, Train Loss: 0.4270, Val Loss: 1.2077\n",
      "Epoch 26/100, Train Loss: 0.4285, Val Loss: 1.1755\n",
      "Epoch 27/100, Train Loss: 0.4294, Val Loss: 1.1474\n",
      "Epoch 28/100, Train Loss: 0.4178, Val Loss: 1.2110\n",
      "Epoch 29/100, Train Loss: 0.3953, Val Loss: 1.1342\n",
      "Epoch 30/100, Train Loss: 0.3786, Val Loss: 1.0596\n",
      "Epoch 31/100, Train Loss: 0.3796, Val Loss: 1.1480\n",
      "Epoch 32/100, Train Loss: 0.3756, Val Loss: 1.2419\n",
      "Epoch 33/100, Train Loss: 0.3645, Val Loss: 1.1058\n",
      "Epoch 34/100, Train Loss: 0.3542, Val Loss: 1.1062\n",
      "Epoch 35/100, Train Loss: 0.3355, Val Loss: 1.0953\n",
      "Epoch 36/100, Train Loss: 0.3192, Val Loss: 1.0954\n",
      "Epoch 37/100, Train Loss: 0.3438, Val Loss: 1.2497\n",
      "Epoch 38/100, Train Loss: 0.3393, Val Loss: 1.1923\n",
      "Epoch 39/100, Train Loss: 0.3056, Val Loss: 1.1646\n",
      "Epoch 40/100, Train Loss: 0.3172, Val Loss: 1.1336\n",
      "Epoch 41/100, Train Loss: 0.2888, Val Loss: 1.1242\n",
      "Epoch 42/100, Train Loss: 0.2735, Val Loss: 1.1187\n",
      "Epoch 43/100, Train Loss: 0.2724, Val Loss: 1.1659\n",
      "Epoch 44/100, Train Loss: 0.2960, Val Loss: 1.1748\n",
      "Epoch 45/100, Train Loss: 0.2562, Val Loss: 1.1675\n",
      "Epoch 46/100, Train Loss: 0.2626, Val Loss: 1.2410\n",
      "Epoch 47/100, Train Loss: 0.2827, Val Loss: 1.2159\n",
      "Epoch 48/100, Train Loss: 0.2465, Val Loss: 1.0512\n",
      "Epoch 49/100, Train Loss: 0.2434, Val Loss: 1.0299\n",
      "Epoch 50/100, Train Loss: 0.2770, Val Loss: 1.1580\n",
      "Epoch 51/100, Train Loss: 0.2412, Val Loss: 1.2428\n",
      "Epoch 52/100, Train Loss: 0.2307, Val Loss: 1.2837\n",
      "Epoch 53/100, Train Loss: 0.2061, Val Loss: 1.1815\n",
      "Epoch 54/100, Train Loss: 0.2110, Val Loss: 1.1390\n",
      "Epoch 55/100, Train Loss: 0.1929, Val Loss: 1.1887\n",
      "Epoch 56/100, Train Loss: 0.1935, Val Loss: 1.1601\n",
      "Epoch 57/100, Train Loss: 0.2061, Val Loss: 1.1579\n",
      "Epoch 58/100, Train Loss: 0.2015, Val Loss: 1.1672\n",
      "Epoch 59/100, Train Loss: 0.1836, Val Loss: 1.1937\n",
      "Epoch 60/100, Train Loss: 0.1857, Val Loss: 1.1569\n",
      "Epoch 61/100, Train Loss: 0.1785, Val Loss: 1.2176\n",
      "Epoch 62/100, Train Loss: 0.1631, Val Loss: 1.2087\n",
      "Epoch 63/100, Train Loss: 0.1684, Val Loss: 1.1445\n",
      "Epoch 64/100, Train Loss: 0.1840, Val Loss: 1.1189\n",
      "Epoch 65/100, Train Loss: 0.1702, Val Loss: 1.1540\n",
      "Epoch 66/100, Train Loss: 0.1691, Val Loss: 1.2932\n",
      "Epoch 67/100, Train Loss: 0.1532, Val Loss: 1.2582\n",
      "Epoch 68/100, Train Loss: 0.1617, Val Loss: 1.2062\n",
      "Epoch 69/100, Train Loss: 0.1547, Val Loss: 1.2185\n",
      "Epoch 70/100, Train Loss: 0.1388, Val Loss: 1.2292\n",
      "Epoch 71/100, Train Loss: 0.1413, Val Loss: 1.1552\n",
      "Epoch 72/100, Train Loss: 0.1528, Val Loss: 1.2656\n",
      "Epoch 73/100, Train Loss: 0.1363, Val Loss: 1.2157\n",
      "Epoch 74/100, Train Loss: 0.1338, Val Loss: 1.2810\n",
      "Epoch 75/100, Train Loss: 0.1264, Val Loss: 1.2327\n",
      "Epoch 76/100, Train Loss: 0.1353, Val Loss: 1.3081\n",
      "Epoch 77/100, Train Loss: 0.1071, Val Loss: 1.2903\n",
      "Epoch 78/100, Train Loss: 0.1159, Val Loss: 1.2521\n",
      "Epoch 79/100, Train Loss: 0.1219, Val Loss: 1.2631\n",
      "Epoch 80/100, Train Loss: 0.1015, Val Loss: 1.3028\n",
      "Epoch 81/100, Train Loss: 0.1162, Val Loss: 1.2972\n",
      "Epoch 82/100, Train Loss: 0.1272, Val Loss: 1.3428\n",
      "Epoch 83/100, Train Loss: 0.1015, Val Loss: 1.3513\n",
      "Epoch 84/100, Train Loss: 0.1028, Val Loss: 1.2107\n",
      "Epoch 85/100, Train Loss: 0.1138, Val Loss: 1.2727\n",
      "Epoch 86/100, Train Loss: 0.0937, Val Loss: 1.3684\n",
      "Epoch 87/100, Train Loss: 0.0982, Val Loss: 1.3333\n",
      "Epoch 88/100, Train Loss: 0.0949, Val Loss: 1.2775\n",
      "Epoch 89/100, Train Loss: 0.0953, Val Loss: 1.3627\n",
      "Epoch 90/100, Train Loss: 0.0902, Val Loss: 1.3043\n",
      "Epoch 91/100, Train Loss: 0.0896, Val Loss: 1.2923\n",
      "Epoch 92/100, Train Loss: 0.0831, Val Loss: 1.2811\n",
      "Epoch 93/100, Train Loss: 0.0786, Val Loss: 1.2964\n",
      "Epoch 94/100, Train Loss: 0.0733, Val Loss: 1.2987\n",
      "Epoch 95/100, Train Loss: 0.0770, Val Loss: 1.3634\n",
      "Epoch 96/100, Train Loss: 0.0751, Val Loss: 1.3038\n",
      "Epoch 97/100, Train Loss: 0.0795, Val Loss: 1.3339\n",
      "Epoch 98/100, Train Loss: 0.0686, Val Loss: 1.4233\n",
      "Epoch 99/100, Train Loss: 0.0674, Val Loss: 1.3897\n",
      "Epoch 100/100, Train Loss: 0.0781, Val Loss: 1.3253\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Flatten X from (batch, 3, 24) to (batch, 72)\n",
    "        batch_x = batch_x.view(batch_x.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        y_recon, mu, logvar = model(batch_x, batch_y)\n",
    "        loss = loss_function(y_recon, batch_y, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * batch_x.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x = batch_x.view(batch_x.size(0), -1)\n",
    "            y_recon, mu, logvar = model(batch_x, batch_y)\n",
    "            loss = loss_function(y_recon, batch_y, mu, logvar)\n",
    "            val_loss += loss.item() * batch_x.size(0)\n",
    "    val_loss /= len(val_dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8dfe54ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Sample:\n",
      "True Y (first sample):\n",
      "tensor([-0.1349,  0.1399, -0.1809,  0.4955,  0.6899,  0.6104,  0.7086,  0.4167,\n",
      "         0.5230, -0.7771, -0.9032, -0.8574,  0.3308, -0.4826, -0.6513, -0.4828,\n",
      "        -0.5125,  0.1398,  0.4041, -0.5161,  0.6285,  0.0626, -0.0558, -0.7194])\n",
      "Predicted Y (first sample):\n",
      "tensor([-0.4228, -0.4991, -0.3475, -0.2164, -0.3202, -0.0194, -0.8071, -0.1626,\n",
      "         0.1505, -0.1739, -0.4011, -0.5874, -0.4829, -0.6422, -0.6498, -0.9500,\n",
      "        -0.7930,  0.1489,  0.0173,  0.1896,  0.0671, -0.0360, -0.0856, -0.9487])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_x, sample_y = next(iter(val_loader))\n",
    "    sample_x = sample_x.view(sample_x.size(0), -1)\n",
    "    y_recon, _, _ = model(sample_x, sample_y)\n",
    "    print(\"\\nValidation Sample:\")\n",
    "    print(\"True Y (first sample):\")\n",
    "    print(sample_y[0])\n",
    "    print(\"Predicted Y (first sample):\")\n",
    "    print(y_recon[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
