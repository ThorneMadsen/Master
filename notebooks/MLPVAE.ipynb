{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import plotly.graph_objects as go\n",
    "from torch.distributions import StudentT\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"../data/X2.npy\")\n",
    "X = X[:855]\n",
    "N, F, T = X.shape\n",
    "\n",
    "target = X[:, 0, :]\n",
    "condition = X[:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"../data/X2.npy\")\n",
    "X = X[:855]\n",
    "N, F, T = X.shape\n",
    "\n",
    "target = X[:, 0, :]\n",
    "condition = X[:, 1:, :]\n",
    "\n",
    "for ch in [0, 1]:\n",
    "    vals = condition[:, ch, :].reshape(-1, 1)\n",
    "    mu = vals.mean()\n",
    "    std = vals.std()\n",
    "    condition[:, ch, :] = (condition[:, ch, :] - mu) / std\n",
    "\n",
    "#mask_bits = np.ones((N, 1, T), dtype=condition.dtype)\n",
    "#condition = np.concatenate([condition, mask_bits], axis=1)\n",
    "\n",
    "cond_tensor = torch.tensor(condition, dtype=torch.float32)\n",
    "target_tensor = torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(cond_tensor, target_tensor)\n",
    "n_total = len(dataset)\n",
    "n_train = int(0.8 * n_total)\n",
    "n_val = n_total - n_train\n",
    "train_dataset, val_dataset = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "cond_dim = condition.shape[1] * condition.shape[2]\n",
    "target_dim = T\n",
    "latent_dim = 32\n",
    "\n",
    "print(f\"cond tensor shape: {cond_tensor.shape}\")\n",
    "print(f\"cond dim: {cond_dim}\")\n",
    "print(f\"target dim: {target_dim}\")\n",
    "print(f\"Total samples: {n_total}, Training samples: {n_train}, Validation samples: {n_val}\")\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "ax = sns.heatmap(target.T, cmap='coolwarm', cbar_kws={'label': 'Imbalance GW'}, xticklabels=50)\n",
    "ax.set_xlabel(\"Day\")\n",
    "ax.set_ylabel(\"Hour\")\n",
    "plt.title(\"Heatmap of Imbalance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_prob):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        #inner_dim = 4 * input_dim \n",
    "        self.ln = nn.LayerNorm(input_dim)\n",
    "        #self.fc1 = nn.Linear(input_dim, inner_dim)\n",
    "        #self.fc2 = nn.Linear(inner_dim, input_dim)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.fc_in = nn.Linear(input_dim, 2 * hidden_dim)\n",
    "        self.proj = nn.Linear(hidden_dim, input_dim)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #residual = x\n",
    "        #x = self.layer_norm(x)\n",
    "        #x = self.fc1(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.fc2(x)\n",
    "        #x = self.dropout(x)\n",
    "        #return x + residual\n",
    "        h = self.ln(x)\n",
    "        h2 = self.fc_in(h)\n",
    "        val, gate = h2.chunk(2, dim=-1)\n",
    "        h3 = val * torch.sigmoid(gate)\n",
    "        h3 = self.act(h3)\n",
    "        out = self.proj(h3)\n",
    "        out = self.dropout(out)\n",
    "        return out + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, cond_dim, target_dim, latent_dim, mlp_blocks, hidden_dim, dropout_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        input_dim = cond_dim + target_dim\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.mlp = nn.Sequential(*[MLPBlock(hidden_dim, hidden_dim * 4, dropout_prob) for _ in range(mlp_blocks)])\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "    \n",
    "    def forward(self, cond, target):\n",
    "        batch_size = cond.shape[0]\n",
    "        cond_flat = cond.view(batch_size, -1)\n",
    "        target_flat = target.view(batch_size, -1)\n",
    "\n",
    "        x = torch.cat([cond_flat, target_flat], dim=1)\n",
    "        x = self.input_layer(x)\n",
    "        x = self.mlp(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, cond_dim, latent_dim, target_dim, mlp_blocks, hidden_dim, dropout_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        #input_dim = cond_dim + latent_dim\n",
    "        self.fc_c = nn.Linear(cond_dim, hidden_dim)\n",
    "        self.fc_z = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        #self.act = nn.ReLU()\n",
    "        # self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.mlp = nn.Sequential(*[MLPBlock(hidden_dim, hidden_dim * 4, dropout_prob) for _ in range(mlp_blocks)])\n",
    "        # self.output_layer = nn.Linear(hidden_dim, target_dim)\n",
    "        self.out_mu     = nn.Linear(hidden_dim, target_dim)\n",
    "        #self.out_logscale = nn.Linear(hidden_dim, target_dim)\n",
    "        self.out_logvar = nn.Linear(hidden_dim, target_dim)\n",
    "        #self.df_raw = nn.Parameter(torch.logit(torch.tensor((5.0 - 2) / (30 - 2))))\n",
    "    \n",
    "    def forward(self, cond, z):\n",
    "        batch_size = cond.shape[0]\n",
    "        cond_flat = cond.view(batch_size, -1)\n",
    "        h_c = self.fc_c(cond_flat)\n",
    "        h_z = self.fc_z(z)\n",
    "        h = self.relu(h_c + h_z)\n",
    "        #h = h + h_z\n",
    "        #h = self.act(self.fc_c(cond_flat) + self.fc_z(z))\n",
    "        h = self.mlp(h)\n",
    "        # x = torch.cat([cond_flat, z], dim=1)\n",
    "        # x = self.input_layer(x)\n",
    "        # x = self.mlp(x)\n",
    "        # out = self.output_layer(h)\n",
    "        # return out\n",
    "        mu_hat = self.out_mu(h)\n",
    "        logvar_hat = self.out_logvar(h)\n",
    "        #logscale = self.out_logscale(h)\n",
    "        #scale_hat = F.softplus(logscale).clamp(min=1e-3, max=0.5)\n",
    "        #df = 5.0 + (100.0 - 2.0) * torch.sigmoid(self.df_raw)\n",
    "        return mu_hat, logvar_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, cond_dim, target_dim, latent_dim, mlp_blocks, hidden_dim, dropout_prob):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.encoder = Encoder(cond_dim, target_dim, latent_dim, mlp_blocks, hidden_dim, dropout_prob)\n",
    "        self.decoder = Decoder(cond_dim, latent_dim, target_dim, mlp_blocks, hidden_dim, dropout_prob)\n",
    "        self.z_dropout = nn.Dropout(p=0.1)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, cond, target):\n",
    "        mu, logvar = self.encoder(cond, target)   \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        #z = self.z_dropout(z)\n",
    "        mu_hat, logvar_hat = self.decoder(cond, z)\n",
    "        \n",
    "        return mu_hat, logvar_hat, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinball_loss(mu_hat, target, tau=0.95):\n",
    "    delta = target - mu_hat\n",
    "    loss = torch.maximum(tau * delta, (tau - 1) * delta)\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crps_gaussian(mu, logvar, target):\n",
    "    \"\"\"CRPS loss for a Gaussian predictive distribution.\"\"\"\n",
    "    std = torch.exp(0.5 * logvar)  # std = exp(0.5 * logvar)\n",
    "    diff = (target - mu) / std\n",
    "\n",
    "    # standard normal PDF and CDF\n",
    "    pdf = torch.exp(-0.5 * diff ** 2) / math.sqrt(2 * math.pi)\n",
    "    cdf = 0.5 * (1 + torch.erf(diff / math.sqrt(2)))\n",
    "\n",
    "    crps = std * (diff * (2 * cdf - 1) + 2 * pdf - 1 / math.sqrt(math.pi))\n",
    "    return crps.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(mu_hat, logvar_hat, target, mu, logvar, kl_weight, min_kl=0.03, return_components=False):\n",
    "    recon_nll = 0.5 * (logvar_hat + (target - mu_hat).pow(2).div(logvar_hat.exp())).mean()\n",
    "    kl_per_dim = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    kl_per_dim = torch.clamp(kl_per_dim, min=min_kl)\n",
    "    kl_loss = kl_per_dim.mean()\n",
    "\n",
    "    total_loss = recon_nll + kl_weight * kl_loss\n",
    "\n",
    "    if return_components:\n",
    "        return total_loss, recon_nll, kl_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_beta = 0.1\n",
    "\n",
    "def calculate_kl_weight(epoch, warmup_epochs, final_beta=final_beta):\n",
    "    if epoch < warmup_epochs:\n",
    "        return (epoch / warmup_epochs) * final_beta\n",
    "    return final_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CVAE(cond_dim, target_dim, latent_dim, mlp_blocks=4, hidden_dim=48, dropout_prob=0.4).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5, weight_decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "warmup_epochs = int(0.10 * num_epochs)\n",
    "mask_ratio = 0.2\n",
    "\n",
    "# Lists to track metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "kl_weights = []\n",
    "recon_losses = []\n",
    "kl_losses = []\n",
    "weighted_kl_losses = []\n",
    "masked_rmses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    kl_weight = calculate_kl_weight(epoch=epoch, warmup_epochs=warmup_epochs)\n",
    "    kl_weights.append(kl_weight)\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    epoch_recon_loss = 0.0\n",
    "    epoch_kl_loss = 0.0\n",
    "    epoch_weighted_kl_loss = 0.0\n",
    "    epoch_masked_se = 0.0\n",
    "    epoch_masked_count = 0\n",
    "    \n",
    "    for batch_cond, batch_target in train_loader:\n",
    "        cond_orig = batch_cond.to(device)\n",
    "        targ_orig = batch_target.to(device)\n",
    "        cond = cond_orig.clone()\n",
    "        targ = targ_orig.clone()\n",
    "\n",
    "        noise_std = 0.1 \n",
    "        cond = cond + torch.randn_like(cond) * noise_std\n",
    "        targ = targ + torch.randn_like(targ) * (noise_std * 0.5)\n",
    "        \n",
    "        #mask = torch.ones_like(targ)\n",
    "        #if torch.rand(1).item() < mask_ratio:\n",
    "            #start = torch.randint(0, T, (1,)).item()\n",
    "            #cond[:, :, start:] = 0.0\n",
    "            #targ[:, start:] = 0.0\n",
    "            #mask[:, start:] = 0.0\n",
    "\n",
    "        \n",
    "        batch_cond = batch_cond.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mu_hat, logvar_hat, mu, logvar = model(cond, targ)\n",
    "        loss, recon_nll, kl_term = loss_function(\n",
    "            mu_hat, logvar_hat, batch_target, mu, logvar, kl_weight, return_components=True\n",
    "        )\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * batch_cond.size(0)\n",
    "        epoch_recon_loss += recon_nll.item() * batch_cond.size(0)\n",
    "        epoch_kl_loss += kl_term.item() * batch_cond.size(0)\n",
    "        epoch_weighted_kl_loss += (kl_weight * kl_term.item()) * batch_cond.size(0)\n",
    "\n",
    "        #with torch.no_grad():\n",
    "            #se = ((mu_hat - targ_orig)**2 * (1 - mask)).sum().item()\n",
    "            #cnt = (1 - mask).sum().item()\n",
    "            #epoch_masked_se += se\n",
    "            #epoch_masked_count += cnt\n",
    "    \n",
    "    #scheduler.step()\n",
    "    \n",
    "    # Average losses over the dataset\n",
    "    avg_train_loss = train_loss / len(train_dataset)\n",
    "    avg_recon_loss = epoch_recon_loss / len(train_dataset)\n",
    "    avg_kl_loss = epoch_kl_loss / len(train_dataset)\n",
    "    avg_weighted_kl_loss = epoch_weighted_kl_loss / len(train_dataset)\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    recon_losses.append(avg_recon_loss)\n",
    "    kl_losses.append(avg_kl_loss)\n",
    "    weighted_kl_losses.append(avg_weighted_kl_loss)\n",
    "\n",
    "    if epoch_masked_count > 0:\n",
    "        masked_rmse = (epoch_masked_se / epoch_masked_count)**0.5\n",
    "    else:\n",
    "        masked_rmse = 0.0\n",
    "    masked_rmses.append(masked_rmse)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_recon_loss = 0.0\n",
    "    val_kl_loss = 0.0\n",
    "    val_weighted_kl_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_cond, batch_target in val_loader:\n",
    "            batch_cond = batch_cond.to(device)\n",
    "            batch_target = batch_target.to(device)\n",
    "            # batch_cond_flat = batch_cond.view(batch_cond.size(0), -1)\n",
    "            mu_hat, logvar_hat, mu, logvar = model(batch_cond, batch_target)\n",
    "            loss, recon_nll_val, kl_term_val = loss_function(\n",
    "                mu_hat, logvar_hat, batch_target, mu, logvar, kl_weight, return_components=True\n",
    "            )\n",
    "            val_loss += loss.item() * batch_cond.size(0)\n",
    "            val_recon_loss += recon_nll_val.item() * batch_cond.size(0)\n",
    "            val_kl_loss += kl_term_val.item() * batch_cond.size(0)\n",
    "            val_weighted_kl_loss += (kl_weight * kl_term_val.item()) * batch_cond.size(0)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_dataset)\n",
    "    avg_val_recon_loss = val_recon_loss / len(val_dataset)\n",
    "    avg_val_kl_loss = val_kl_loss / len(val_dataset)\n",
    "    avg_val_weighted_kl_loss = val_weighted_kl_loss / len(val_dataset)\n",
    "    \n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, KL Weight: {kl_weight:.4f}\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Recon Loss: {avg_recon_loss:.4f}, KL Loss: {avg_kl_loss:.4f}, Weighted KL: {avg_weighted_kl_loss:.4f}\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}, Val Recon Loss: {avg_val_recon_loss:.4f}, Val KL Loss: {avg_val_kl_loss:.4f}, Val Weighted KL: {avg_val_weighted_kl_loss:.4f}\")\n",
    "        # print(f\"Masked‐region RMSE: {masked_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(masked_rmses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Masked RMSE\")\n",
    "plt.title(\"Imputation Error on Masked Regions Over Training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    total_kl = 0\n",
    "    for cond, tar in val_loader:\n",
    "       mu, logvar = model.encoder(cond.view(cond.size(0),-1), tar)\n",
    "       kl = (-0.5 * (1 + logvar - mu.pow(2) - logvar.exp())).mean(dim=0)\n",
    "       total_kl += kl\n",
    "    avg_kl = total_kl / len(val_loader)\n",
    "print(avg_kl.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot 1: KL weight over time\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(kl_weights)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('KL Weight')\n",
    "plt.title('KL Annealing Schedule')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 2: Total losses over time\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Total Loss over Training')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 3: Reconstruction loss over time\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot(recon_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Reconstruction Loss')\n",
    "plt.title('Reconstruction Loss over Training')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 4: Raw KL divergence over time\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(kl_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('KL Divergence')\n",
    "plt.title('Raw KL Divergence (unweighted)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 5: Weighted KL divergence over time\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.plot(weighted_kl_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Weighted KL Loss')\n",
    "plt.title('Weighted KL Divergence')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 6: Ratio of KL to reconstruction loss\n",
    "plt.subplot(3, 2, 6)\n",
    "ratio = [kl/recon if recon > 0 else 0 for kl, recon in zip(kl_losses, recon_losses)]\n",
    "plt.plot(ratio)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('KL / Reconstruction Ratio')\n",
    "plt.title('Ratio of KL Divergence to Reconstruction Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doy_and_dow_from_validation(val_dataset, val_index):\n",
    "    sample_cond, sample_target = val_dataset[val_index]\n",
    "    target_numpy = sample_target.numpy()\n",
    "\n",
    "    day_sin = sample_cond[5, 0].item()\n",
    "    day_cos = sample_cond[6, 0].item()\n",
    "    week_sin = sample_cond[7, 0].item()\n",
    "    week_cos = sample_cond[8, 0].item()\n",
    "\n",
    "    day_rad = np.arctan2(day_sin, day_cos)\n",
    "    day_angle = day_rad if day_rad >= 0 else day_rad + (2 * np.pi)\n",
    "    day_of_year_float = (day_angle / (2 * np.pi)) * 365.25\n",
    "    day_of_year = int(np.clip(round(day_of_year_float), 1, 365))\n",
    "\n",
    "    week_rad = np.arctan2(week_sin, week_cos)\n",
    "    week_angle = week_rad if week_rad >= 0 else week_rad + (2 * np.pi)\n",
    "    day_of_week_float = (week_angle / (2 * np.pi)) * 7.0\n",
    "    day_of_week = int(round(day_of_week_float) % 7)\n",
    "\n",
    "    return day_of_year, day_of_week, target_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_week_earlier_target_with_dow(current_doy, current_dow, X):\n",
    "    target_dow = current_dow % 7\n",
    "    target_doy = (current_doy - 1 - 7) % 365 + 1\n",
    "\n",
    "    all_week_sin, all_week_cos = X[:, 7, 0], X[:, 8, 0]\n",
    "    all_week_rad = np.arctan2(all_week_sin, all_week_cos)\n",
    "    all_week_angle = np.where(all_week_rad >= 0, all_week_rad, all_week_rad + (2 * np.pi))\n",
    "    all_calculated_dow = np.round((all_week_angle / (2 * np.pi)) * 7.0).astype(int) % 7\n",
    "\n",
    "    dow_match_indices = np.where(all_calculated_dow == target_dow)[0]\n",
    "    if len(dow_match_indices) == 0: return None\n",
    "\n",
    "    subset_day_sin = X[dow_match_indices, 5, 0]\n",
    "    subset_day_cos = X[dow_match_indices, 6, 0]\n",
    "    subset_day_rad = np.arctan2(subset_day_sin, subset_day_cos)\n",
    "    subset_day_angle = np.where(subset_day_rad >= 0, subset_day_rad, subset_day_rad + (2 * np.pi))\n",
    "    subset_doy_float = (subset_day_angle / (2 * np.pi)) * 365.25\n",
    "    subset_calculated_doy = np.clip(np.round(subset_doy_float), 1, 365).astype(int)\n",
    "\n",
    "    diff = np.abs(subset_calculated_doy - target_doy)\n",
    "    min_diff = np.minimum(diff, 365 - diff)\n",
    "    best_subset_idx = np.argmin(min_diff)\n",
    "    original_idx = dow_match_indices[best_subset_idx]\n",
    "\n",
    "    return X[original_idx, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seven_days_before_target(target_output, X):\n",
    "    all_targets = X[:, 0, :]\n",
    "\n",
    "    matches = []\n",
    "    for i in range(len(all_targets)):\n",
    "        if np.allclose(all_targets[i], target_output, rtol=1e-5, atol=1e-5):\n",
    "            matches.append(i)\n",
    "    \n",
    "    if not matches:\n",
    "        print(\"Target not found in dataset X\")\n",
    "        return None\n",
    "    \n",
    "    target_idx = matches[0]\n",
    "\n",
    "    if target_idx < 7:\n",
    "        print(f\"Not enough days before target (index: {target_idx})\")\n",
    "        return None\n",
    "    \n",
    "    seven_days_before = all_targets[target_idx-7:target_idx]\n",
    "    return seven_days_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_directional_improvement(target_data, prediction_data, positive=True, label=\"Prediction\"):\n",
    "    mask = target_data > 0 if positive else target_data < 0\n",
    "    direction = \"positive\" if positive else \"negative\"\n",
    "    \n",
    "    filtered_indices = np.where(mask)[0]\n",
    "    filtered_target = target_data[mask]\n",
    "    \n",
    "    if len(filtered_target) == 0:\n",
    "        print(f\"  No {direction} values found in target data.\")\n",
    "        return False\n",
    "    \n",
    "    filtered_prediction = prediction_data[filtered_indices]\n",
    "    \n",
    "    if positive:\n",
    "        comparison_count = np.sum(filtered_target > filtered_prediction)\n",
    "        comparison_text = \"Target > Prediction\"\n",
    "    else:\n",
    "        comparison_count = np.sum(filtered_target < filtered_prediction)\n",
    "        comparison_text = \"Target < Prediction\"\n",
    "    \n",
    "    total_points = len(filtered_target)\n",
    "    percentage = (comparison_count / total_points * 100) if total_points > 0 else 0\n",
    "    abs_diff = np.mean(np.abs(filtered_target - filtered_prediction))\n",
    "    \n",
    "    print(f\"  {direction.capitalize()} Values Metrics for {label}:\")\n",
    "    print(f\"    {comparison_text}: {percentage:.2f}% ({comparison_count}/{total_points})\")\n",
    "    print(f\"    Avg Absolute Diff: {abs_diff:.4f}\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doy, dow, target_data = get_doy_and_dow_from_validation(val_dataset, 0)\n",
    "print(f\"\\nCalculated - DoY: {doy}, DoW: {dow}\")\n",
    "print(f\"Target Data: {target_data}\")\n",
    "\n",
    "daily_samples = get_seven_days_before_target(target_data, X)\n",
    "print(daily_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_sample_generation_with_week_earlier(model, X, val_dataset, latent_dim, val_index, num_samples=100):\n",
    "    doy, dow, target_data = get_doy_and_dow_from_validation(val_dataset, val_index)\n",
    "    week_earlier_data = find_week_earlier_target_with_dow(doy, dow, X)\n",
    "    if week_earlier_data is not None: print(f\"Week Earlier Data Found:\\n{week_earlier_data[:5]}...\\n{'-'*20}\")\n",
    "    else: print(f\"Week Earlier Data Not Found.\\n{'-'*20}\")\n",
    "\n",
    "    scalar_mean_target = np.mean(X[:, 0, :])\n",
    "    mean_line_data = np.full(target_data.shape[0], scalar_mean_target)\n",
    "\n",
    "    sample_cond, _ = val_dataset[val_index]\n",
    "    cond_flat = sample_cond.unsqueeze(0).to(device).view(1, -1)\n",
    "    cond_batch = sample_cond.unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    samples_list = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            z = torch.randn(1, latent_dim).to(device)\n",
    "            mu_hat, log_scale_hat, df = model.decoder(cond_batch, z)\n",
    "            scale = torch.exp(log_scale_hat)\n",
    "            dist  = StudentT(df, loc=mu_hat, scale=scale)\n",
    "            recon = dist.sample()\n",
    "            samples_list.append(recon.cpu().numpy().flatten())\n",
    "            #mu_hat, logvar_hat = model.decoder(cond_flat, z)\n",
    "            #eps = torch.randn_like(logvar_hat)\n",
    "            #recon = mu_hat + eps * torch.exp(0.5 * logvar_hat)\n",
    "            #samples_list.append(recon.cpu().numpy().flatten())\n",
    "\n",
    "    traces = [\n",
    "        go.Scatter(y=target_data, name=\"Target\", line=dict(color='black', width=2)),\n",
    "        go.Scatter(y=mean_line_data, name=\"Overall Mean\", line=dict(color='grey', width=2, dash='dash')),\n",
    "        go.Scatter(y=week_earlier_data, name=\"Week Earlier\", line=dict(color='red', width=2))\n",
    "    ]\n",
    "    for i, sample in enumerate(samples_list):\n",
    "         traces.append(go.Scatter(y=sample, name=f\"Sample {i+1}\", showlegend=False, line=dict(color='blue', width=1), opacity=0.4))\n",
    "\n",
    "    fig = go.Figure(data=traces)\n",
    "    fig.update_layout(title=f\"Val Idx {val_index}: Target, Overall Mean, Generations & Week Earlier\", xaxis_title=\"Hour\", yaxis_title=\"Value\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_val_sample_generation_with_week_earlier(model, X, val_dataset, latent_dim, 2)\n",
    "if fig: fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_with_std_band(model, X, val_dataset, latent_dim, val_index, num_samples=10000):\n",
    "    _, _, target_data = get_doy_and_dow_from_validation(val_dataset, val_index)\n",
    "    seven_days_before = get_seven_days_before_target(target_data, X)\n",
    "    if seven_days_before is None: return None\n",
    "    \n",
    "    past_mean = np.mean(seven_days_before, axis=0)\n",
    "    past_std = np.std(seven_days_before, axis=0)\n",
    "    \n",
    "    max_imbalance = np.max(X[:, 0, :])\n",
    "    min_imbalance = np.min(X[:, 0, :])\n",
    "    max_line_data = np.full(target_data.shape[0], max_imbalance)\n",
    "    min_line_data = np.full(target_data.shape[0], min_imbalance)\n",
    "    \n",
    "    sample_cond, _ = val_dataset[val_index]\n",
    "    cond_flat = sample_cond.unsqueeze(0).to(device).view(1, -1)\n",
    "    cond_batch = sample_cond.unsqueeze(0).to(device)\n",
    "\n",
    "    samples_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            z = torch.randn(1, latent_dim).to(device)\n",
    "            mu_hat, logvar_hat = model.decoder(cond_flat, z)\n",
    "            eps = torch.randn_like(logvar_hat)\n",
    "            recon = mu_hat + eps * torch.exp(0.5 * logvar_hat)\n",
    "            samples_list.append(recon.cpu().numpy().flatten())\n",
    "            # Student-t sampling\n",
    "            #mu_hat, log_scale_hat, df = model.decoder(cond_batch, z)\n",
    "            #scale = torch.exp(log_scale_hat)\n",
    "            #dist  = StudentT(df, loc=mu_hat, scale=scale)\n",
    "            #recon = dist.sample()\n",
    "            #samples_list.append(recon.cpu().numpy().flatten())\n",
    "\n",
    "    samples_array = np.array(samples_list)\n",
    "    percentile_95 = np.percentile(samples_array, 95, axis=0)\n",
    "    percentile_5 = np.percentile(samples_array, 5, axis=0)\n",
    "\n",
    "    print(f\"--- Evaluation for Val Idx {val_index} ---\")\n",
    "    evaluate_directional_improvement(target_data, percentile_95, positive=True, label=\"95th Percentile\")\n",
    "    print(\"-\" * 20)\n",
    "    evaluate_directional_improvement(target_data, percentile_5, positive=False, label=\"5th Percentile\")\n",
    "    print(\"-\" * 20)\n",
    "    evaluate_directional_improvement(target_data, max_line_data, positive=True, label=\"Max Value in X\")\n",
    "    print(\"-\" * 20)\n",
    "    evaluate_directional_improvement(target_data, min_line_data, positive=False, label=\"Min Value in X\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    hours = list(range(24))\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=hours, y=target_data, name=\"Target\", line=dict(color='black', width=2)))\n",
    "    fig.add_trace(go.Scatter(x=hours, y=min_line_data, name=\"Overall Min\", line=dict(color='grey', width=2, dash='dash')))\n",
    "    fig.add_trace(go.Scatter(x=hours, y=max_line_data, name=\"Overall Max\", line=dict(color='grey', width=2, dash='dash')))\n",
    "    fig.add_trace(go.Scatter(x=hours, y=percentile_95, name=\"95th Percentile\", line=dict(color='blue', width=2)))\n",
    "    fig.add_trace(go.Scatter(x=hours, y=percentile_5, name=\"5th Percentile\", line=dict(color='blue', width=2)))\n",
    "    fig.add_trace(go.Scatter(x=hours, y=past_mean, name=\"Mean of Past 7 Days\", line=dict(color='red', width=2)))\n",
    "    \n",
    "    #fig.add_trace(go.Scatter(\n",
    "    #    x=hours + hours[::-1],\n",
    "    #    y=np.concatenate([past_mean + past_std, (past_mean - past_std)[::-1]]),\n",
    "    #    fill='toself',\n",
    "    #    fillcolor='rgba(255,0,0,0.2)',\n",
    "    #    line=dict(color='rgba(255,0,0,0)'),\n",
    "    #    name='Mean ± 1 StdDev'\n",
    "    #))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=hours + hours[::-1],\n",
    "        y=np.concatenate([past_mean + 2*past_std, (past_mean - 2*past_std)[::-1]]),\n",
    "        fill='toself',\n",
    "        fillcolor='rgba(255,0,0,0.1)',\n",
    "        line=dict(color='rgba(255,0,0,0)'),\n",
    "        name='Mean + 2*StdDev'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(title=f\"Val Idx {val_index}\", xaxis_title=\"Hour\", yaxis_title=\"Value\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_val_with_std_band(model, X, val_dataset, latent_dim, 4, 1000)\n",
    "if fig: fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_metrics(model, X, val_dataset, latent_dim, num_samples=10000):\n",
    "    metrics = {\n",
    "        \"95th_percentile\": {\"positive\": {\"percentage\": [], \"abs_diff\": []}, \"count\": 0},\n",
    "        \"5th_percentile\": {\"negative\": {\"percentage\": [], \"abs_diff\": []}, \"count\": 0},\n",
    "        \"past_mean_plus_2std\": {\"positive\": {\"percentage\": [], \"abs_diff\": []}, \"count\": 0},\n",
    "        \"past_mean_minus_2std\": {\"negative\": {\"percentage\": [], \"abs_diff\": []}, \"count\": 0},\n",
    "        \"max_value\": {\"positive\": {\"percentage\": [], \"abs_diff\": []}, \"count\": 0},\n",
    "        \"min_value\": {\"negative\": {\"percentage\": [], \"abs_diff\": []}, \"count\": 0}\n",
    "    }\n",
    "    \n",
    "    samples_processed = 0\n",
    "    \n",
    "    for val_index in range(len(val_dataset)):\n",
    "        _, _, target_data = get_doy_and_dow_from_validation(val_dataset, val_index)\n",
    "        seven_days_before = get_seven_days_before_target(target_data, X)\n",
    "        if seven_days_before is None: continue\n",
    "        \n",
    "        past_mean = np.mean(seven_days_before, axis=0)\n",
    "        past_std = np.std(seven_days_before, axis=0)\n",
    "        \n",
    "        past_mean_plus_2std = past_mean + 2*past_std\n",
    "        past_mean_minus_2std = past_mean - 2*past_std\n",
    "        \n",
    "        max_imbalance = np.max(X[:, 0, :])\n",
    "        min_imbalance = np.min(X[:, 0, :])\n",
    "        max_line_data = np.full(target_data.shape[0], max_imbalance)\n",
    "        min_line_data = np.full(target_data.shape[0], min_imbalance)\n",
    "        \n",
    "        sample_cond, _ = val_dataset[val_index]\n",
    "        cond_flat = sample_cond.unsqueeze(0).to(device).view(1, -1)\n",
    "        cond_batch = sample_cond.unsqueeze(0).to(device)\n",
    "\n",
    "        samples_list = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_samples):\n",
    "                z = torch.randn(1, latent_dim).to(device)\n",
    "                mu_hat, logvar_hat = model.decoder(cond_flat, z)\n",
    "                eps = torch.randn_like(logvar_hat)\n",
    "                recon = mu_hat + eps * torch.exp(0.5 * logvar_hat)\n",
    "                samples_list.append(recon.cpu().numpy().flatten())\n",
    "                #mu_hat, log_scale_hat, df = model.decoder(cond_batch, z)\n",
    "                #scale = torch.exp(log_scale_hat)\n",
    "                #dist  = StudentT(df, loc=mu_hat, scale=scale)\n",
    "                #recon = dist.sample()\n",
    "                #samples_list.append(recon.cpu().numpy().flatten())\n",
    "\n",
    "        samples_array = np.array(samples_list)\n",
    "        percentile_95 = np.percentile(samples_array, 95, axis=0)\n",
    "        percentile_5 = np.percentile(samples_array, 5, axis=0)\n",
    "\n",
    "        # Collect metrics\n",
    "        # 95th percentile (positive values)\n",
    "        mask_positive = target_data > 0\n",
    "        if np.any(mask_positive):\n",
    "            filtered_indices = np.where(mask_positive)[0]\n",
    "            filtered_target = target_data[mask_positive]\n",
    "            filtered_prediction = percentile_95[filtered_indices]\n",
    "            \n",
    "            comparison_count = np.sum(filtered_target > filtered_prediction)\n",
    "            total_points = len(filtered_target)\n",
    "            percentage = (comparison_count / total_points * 100) if total_points > 0 else 0\n",
    "            abs_diff = np.mean(np.abs(filtered_target - filtered_prediction))\n",
    "            \n",
    "            metrics[\"95th_percentile\"][\"positive\"][\"percentage\"].append(percentage)\n",
    "            metrics[\"95th_percentile\"][\"positive\"][\"abs_diff\"].append(abs_diff)\n",
    "            metrics[\"95th_percentile\"][\"count\"] += 1\n",
    "        \n",
    "        # 5th percentile (negative values)\n",
    "        mask_negative = target_data < 0\n",
    "        if np.any(mask_negative):\n",
    "            filtered_indices = np.where(mask_negative)[0]\n",
    "            filtered_target = target_data[mask_negative]\n",
    "            filtered_prediction = percentile_5[filtered_indices]\n",
    "            \n",
    "            comparison_count = np.sum(filtered_target < filtered_prediction)\n",
    "            total_points = len(filtered_target)\n",
    "            percentage = (comparison_count / total_points * 100) if total_points > 0 else 0\n",
    "            abs_diff = np.mean(np.abs(filtered_target - filtered_prediction))\n",
    "            \n",
    "            metrics[\"5th_percentile\"][\"negative\"][\"percentage\"].append(percentage)\n",
    "            metrics[\"5th_percentile\"][\"negative\"][\"abs_diff\"].append(abs_diff)\n",
    "            metrics[\"5th_percentile\"][\"count\"] += 1\n",
    "        \n",
    "        # Max value (positive values)\n",
    "        if np.any(mask_positive):\n",
    "            filtered_indices = np.where(mask_positive)[0]\n",
    "            filtered_target = target_data[mask_positive]\n",
    "            filtered_prediction = max_line_data[filtered_indices]\n",
    "\n",
    "            tolerance = 1e-7\n",
    "            comparison_count = np.sum(filtered_target - filtered_prediction > tolerance)\n",
    "            total_points = len(filtered_target)\n",
    "            percentage = (comparison_count / total_points * 100) if total_points > 0 else 0\n",
    "            abs_diff = np.mean(np.abs(filtered_target - filtered_prediction))\n",
    "            \n",
    "            metrics[\"max_value\"][\"positive\"][\"percentage\"].append(percentage)\n",
    "            metrics[\"max_value\"][\"positive\"][\"abs_diff\"].append(abs_diff)\n",
    "            metrics[\"max_value\"][\"count\"] += 1\n",
    "        \n",
    "        # Min value (negative values)\n",
    "        if np.any(mask_negative):\n",
    "            filtered_indices = np.where(mask_negative)[0]\n",
    "            filtered_target = target_data[mask_negative]\n",
    "            filtered_prediction = min_line_data[filtered_indices]\n",
    "            \n",
    "            tolerance = 1e-7\n",
    "            comparison_count = np.sum(filtered_target - filtered_prediction < -tolerance)\n",
    "            total_points = len(filtered_target)\n",
    "            percentage = (comparison_count / total_points * 100) if total_points > 0 else 0\n",
    "            abs_diff = np.mean(np.abs(filtered_target - filtered_prediction))\n",
    "            \n",
    "            metrics[\"min_value\"][\"negative\"][\"percentage\"].append(percentage)\n",
    "            metrics[\"min_value\"][\"negative\"][\"abs_diff\"].append(abs_diff)\n",
    "            metrics[\"min_value\"][\"count\"] += 1\n",
    "            \n",
    "        # Past mean + 2*std (positive values)\n",
    "        if np.any(mask_positive):\n",
    "            filtered_indices = np.where(mask_positive)[0]\n",
    "            filtered_target = target_data[mask_positive]\n",
    "            filtered_prediction = past_mean_plus_2std[filtered_indices]\n",
    "            \n",
    "            comparison_count = np.sum(filtered_target > filtered_prediction)\n",
    "            total_points = len(filtered_target)\n",
    "            percentage = (comparison_count / total_points * 100) if total_points > 0 else 0\n",
    "            abs_diff = np.mean(np.abs(filtered_target - filtered_prediction))\n",
    "            \n",
    "            metrics[\"past_mean_plus_2std\"][\"positive\"][\"percentage\"].append(percentage)\n",
    "            metrics[\"past_mean_plus_2std\"][\"positive\"][\"abs_diff\"].append(abs_diff)\n",
    "            metrics[\"past_mean_plus_2std\"][\"count\"] += 1\n",
    "            \n",
    "        # Past mean - 2*std (negative values)\n",
    "        if np.any(mask_negative):\n",
    "            filtered_indices = np.where(mask_negative)[0]\n",
    "            filtered_target = target_data[mask_negative]\n",
    "            filtered_prediction = past_mean_minus_2std[filtered_indices]\n",
    "            \n",
    "            comparison_count = np.sum(filtered_target < filtered_prediction)\n",
    "            total_points = len(filtered_target)\n",
    "            percentage = (comparison_count / total_points * 100) if total_points > 0 else 0\n",
    "            abs_diff = np.mean(np.abs(filtered_target - filtered_prediction))\n",
    "            \n",
    "            metrics[\"past_mean_minus_2std\"][\"negative\"][\"percentage\"].append(percentage)\n",
    "            metrics[\"past_mean_minus_2std\"][\"negative\"][\"abs_diff\"].append(abs_diff)\n",
    "            metrics[\"past_mean_minus_2std\"][\"count\"] += 1\n",
    "        \n",
    "        samples_processed += 1\n",
    "    \n",
    "    # Calculate averages\n",
    "    results = {}\n",
    "    \n",
    "    for metric_name, metric_data in metrics.items():\n",
    "        if \"positive\" in metric_data:\n",
    "            if metric_data[\"count\"] > 0:\n",
    "                avg_percentage = np.mean(metric_data[\"positive\"][\"percentage\"])\n",
    "                avg_abs_diff = np.mean(metric_data[\"positive\"][\"abs_diff\"])\n",
    "                results[f\"{metric_name}_positive_percentage\"] = avg_percentage\n",
    "                results[f\"{metric_name}_positive_abs_diff\"] = avg_abs_diff\n",
    "        \n",
    "        if \"negative\" in metric_data:\n",
    "            if metric_data[\"count\"] > 0:\n",
    "                avg_percentage = np.mean(metric_data[\"negative\"][\"percentage\"])\n",
    "                avg_abs_diff = np.mean(metric_data[\"negative\"][\"abs_diff\"])\n",
    "                results[f\"{metric_name}_negative_percentage\"] = avg_percentage\n",
    "                results[f\"{metric_name}_negative_abs_diff\"] = avg_abs_diff\n",
    "    \n",
    "    results[\"samples_processed\"] = samples_processed\n",
    "    \n",
    "    print(f\"Processed {samples_processed} validation samples\")\n",
    "    print(\"\\nAverage Metrics:\")\n",
    "    for key, value in results.items():\n",
    "        if key != \"samples_processed\":\n",
    "            print(f\"  {key}: {value:.2f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_average_metrics(model, X, val_dataset, latent_dim, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_with_masked_hours(model, val_dataset, latent_dim, val_index, keep_hour=None, num_samples=10000):\n",
    "    _, _, target_data = get_doy_and_dow_from_validation(val_dataset, val_index)\n",
    "    if target_data is None: return None\n",
    "    \n",
    "    sample_cond, _ = val_dataset[val_index]\n",
    "    masked_cond = sample_cond.clone()\n",
    "    \n",
    "    if keep_hour is not None and 0 <= keep_hour < masked_cond.shape[1]:\n",
    "        for hour_idx in range(masked_cond.shape[1]):\n",
    "            if hour_idx > keep_hour:\n",
    "                masked_cond[:, hour_idx] = 0.0\n",
    "    \n",
    "    cond_flat = masked_cond.unsqueeze(0).to(device).view(1, -1)\n",
    "    \n",
    "    samples_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            z = torch.randn(1, latent_dim).to(device)\n",
    "            mu_hat, logvar_hat = model.decoder(cond_flat, z)\n",
    "            eps = torch.randn_like(logvar_hat)\n",
    "            recon = mu_hat + eps * torch.exp(0.5 * logvar_hat)\n",
    "            samples_list.append(recon.cpu().numpy().flatten())\n",
    "    \n",
    "    samples_array = np.array(samples_list)\n",
    "    percentile_95 = np.percentile(samples_array, 95, axis=0)\n",
    "    percentile_5 = np.percentile(samples_array, 5, axis=0)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(f\"--- Evaluation for Val Idx {val_index} with Hour Masking ---\")\n",
    "    evaluate_directional_improvement(target_data, percentile_95, positive=True, label=\"95th Percentile\")\n",
    "    print(\"-\" * 20)\n",
    "    evaluate_directional_improvement(target_data, percentile_5, positive=False, label=\"5th Percentile\")\n",
    "    \n",
    "    hours = list(range(24))\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=hours, y=target_data, name=\"Target\", line=dict(color='black', width=2)))\n",
    "    fig.add_trace(go.Scatter(x=hours, y=percentile_95, name=\"95th Percentile\", line=dict(color='blue', width=2)))\n",
    "    fig.add_trace(go.Scatter(x=hours, y=percentile_5, name=\"5th Percentile\", line=dict(color='blue', width=2)))\n",
    "    \n",
    "    # Update layout\n",
    "    mask_str = f\", Up to hour {keep_hour+1} Kept\"\n",
    "    fig.update_layout(\n",
    "        title=f\"Val Idx {val_index}{mask_str}\", \n",
    "        xaxis_title=\"Hour\", \n",
    "        yaxis_title=\"Value\"\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_with_masked_hours(model, val_dataset, latent_dim, val_index=1, keep_hour=24, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentile_metrics_with_hourly_graph(model, X, val_dataset, latent_dim, num_samples=10000):\n",
    "    metrics = {\n",
    "        \"95th_percentile\": {\"positive\": {\"percentage\": [], \"abs_diff\": [], \"hour_errors\": [[] for _ in range(24)]}, \"count\": 0},\n",
    "        \"5th_percentile\": {\"negative\": {\"percentage\": [], \"abs_diff\": [], \"hour_errors\": [[] for _ in range(24)]}, \"count\": 0}\n",
    "    }\n",
    "    \n",
    "    samples_processed = 0\n",
    "    \n",
    "    for val_index in range(len(val_dataset)):\n",
    "        doy, dow, target_data = get_doy_and_dow_from_validation(val_dataset, val_index)\n",
    "        seven_days_before = get_seven_days_before_target(target_data, X)\n",
    "        if seven_days_before is None: continue\n",
    "        \n",
    "        sample_cond, _ = val_dataset[val_index]\n",
    "        cond_flat = sample_cond.unsqueeze(0).to(device).view(1, -1)\n",
    "\n",
    "        samples_list = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_samples):\n",
    "                z = torch.randn(1, latent_dim).to(device)\n",
    "                mu_hat, logvar_hat = model.decoder(cond_flat, z)\n",
    "                eps = torch.randn_like(logvar_hat)\n",
    "                recon = mu_hat + eps * torch.exp(0.5 * logvar_hat)\n",
    "                samples_list.append(recon.cpu().numpy().flatten())\n",
    "\n",
    "        samples_array = np.array(samples_list)\n",
    "        percentile_95 = np.percentile(samples_array, 95, axis=0)\n",
    "        percentile_5 = np.percentile(samples_array, 5, axis=0)\n",
    "\n",
    "        mask_positive = target_data > 0\n",
    "        if np.any(mask_positive):\n",
    "            filtered_indices = np.where(mask_positive)[0]\n",
    "            filtered_target = target_data[mask_positive]\n",
    "            filtered_prediction = percentile_95[filtered_indices]\n",
    "            \n",
    "            comparison_count = np.sum(filtered_target > filtered_prediction)\n",
    "            total_points = len(filtered_target)\n",
    "            percentage = (comparison_count / total_points * 100) if total_points > 0 else 0\n",
    "            abs_diff = np.mean(np.abs(filtered_target - filtered_prediction))\n",
    "            \n",
    "            for i, idx in enumerate(filtered_indices):\n",
    "                hour = idx % 24\n",
    "                error = abs(filtered_target[i] - filtered_prediction[i])\n",
    "                metrics[\"95th_percentile\"][\"positive\"][\"hour_errors\"][hour].append(error)\n",
    "            \n",
    "            metrics[\"95th_percentile\"][\"positive\"][\"percentage\"].append(percentage)\n",
    "            metrics[\"95th_percentile\"][\"positive\"][\"abs_diff\"].append(abs_diff)\n",
    "            metrics[\"95th_percentile\"][\"count\"] += 1\n",
    "        \n",
    "        mask_negative = target_data < 0\n",
    "        if np.any(mask_negative):\n",
    "            filtered_indices = np.where(mask_negative)[0]\n",
    "            filtered_target = target_data[mask_negative]\n",
    "            filtered_prediction = percentile_5[filtered_indices]\n",
    "            \n",
    "            comparison_count = np.sum(filtered_target < filtered_prediction)\n",
    "            total_points = len(filtered_target)\n",
    "            percentage = (comparison_count / total_points * 100) if total_points > 0 else 0\n",
    "            abs_diff = np.mean(np.abs(filtered_target - filtered_prediction))\n",
    "            \n",
    "            for i, idx in enumerate(filtered_indices):\n",
    "                hour = idx % 24\n",
    "                error = abs(filtered_target[i] - filtered_prediction[i])\n",
    "                metrics[\"5th_percentile\"][\"negative\"][\"hour_errors\"][hour].append(error)\n",
    "            \n",
    "            metrics[\"5th_percentile\"][\"negative\"][\"percentage\"].append(percentage)\n",
    "            metrics[\"5th_percentile\"][\"negative\"][\"abs_diff\"].append(abs_diff)\n",
    "            metrics[\"5th_percentile\"][\"count\"] += 1\n",
    "        \n",
    "        samples_processed += 1\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for metric_name, metric_data in metrics.items():\n",
    "        if \"positive\" in metric_data and metric_data[\"count\"] > 0:\n",
    "            results[f\"{metric_name}_positive_percentage\"] = np.mean(metric_data[\"positive\"][\"percentage\"])\n",
    "            results[f\"{metric_name}_positive_abs_diff\"] = np.mean(metric_data[\"positive\"][\"abs_diff\"])\n",
    "        \n",
    "        if \"negative\" in metric_data and metric_data[\"count\"] > 0:\n",
    "            results[f\"{metric_name}_negative_percentage\"] = np.mean(metric_data[\"negative\"][\"percentage\"])\n",
    "            results[f\"{metric_name}_negative_abs_diff\"] = np.mean(metric_data[\"negative\"][\"abs_diff\"])\n",
    "    \n",
    "    results[\"samples_processed\"] = samples_processed\n",
    "    \n",
    "    print(f\"Processed {samples_processed} validation samples\")\n",
    "    print(\"\\nAverage Metrics:\")\n",
    "    for key, value in results.items():\n",
    "        if key != \"samples_processed\":\n",
    "            print(f\"  {key}: {value:.2f}\")\n",
    "    \n",
    "    hourly_data = {\"95th_percentile_positive\": [], \"5th_percentile_negative\": []}\n",
    "    \n",
    "    for hour in range(24):\n",
    "        hourly_data[\"95th_percentile_positive\"].append(np.mean(metrics[\"95th_percentile\"][\"positive\"][\"hour_errors\"][hour]) if metrics[\"95th_percentile\"][\"positive\"][\"hour_errors\"][hour] else 0)\n",
    "        hourly_data[\"5th_percentile_negative\"].append(np.mean(metrics[\"5th_percentile\"][\"negative\"][\"hour_errors\"][hour]) if metrics[\"5th_percentile\"][\"negative\"][\"hour_errors\"][hour] else 0)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(24), hourly_data[\"95th_percentile_positive\"], 'o-', color='red', linewidth=2, markersize=8)\n",
    "    plt.title('95th Percentile (Positive Values) - Mean Absolute Error by Hour')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(range(24))\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(range(24), hourly_data[\"5th_percentile_negative\"], 'o-', color='blue', linewidth=2, markersize=8)\n",
    "    plt.title('5th Percentile (Negative Values) - Mean Absolute Error by Hour')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(range(24))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    x = np.arange(24)\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, hourly_data[\"95th_percentile_positive\"], width, label='95th Percentile (Positive)', color='red', alpha=0.7)\n",
    "    plt.bar(x + width/2, hourly_data[\"5th_percentile_negative\"], width, label='5th Percentile (Negative)', color='blue', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.title('Comparison of 5th and 95th Percentile Errors by Hour')\n",
    "    plt.xticks(range(24))\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sample_counts = {\n",
    "        \"positive_samples_per_hour\": [len(metrics[\"95th_percentile\"][\"positive\"][\"hour_errors\"][h]) for h in range(24)],\n",
    "        \"negative_samples_per_hour\": [len(metrics[\"5th_percentile\"][\"negative\"][\"hour_errors\"][h]) for h in range(24)]\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.bar(x - width/2, sample_counts[\"positive_samples_per_hour\"], width, label='Positive Samples', color='darkred', alpha=0.7)\n",
    "    plt.bar(x + width/2, sample_counts[\"negative_samples_per_hour\"], width, label='Negative Samples', color='darkblue', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Sample Distribution by Hour')\n",
    "    plt.xticks(range(24))\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results, hourly_data, sample_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, hourly_data, sample_counts = calculate_percentile_metrics_with_hourly_graph(model, X, val_dataset, latent_dim, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_seasonal_hourly_metrics(model, X, val_dataset, latent_dim, num_samples=10000):\n",
    "    month_hour_errors = {\"95th_percentile\": np.zeros((12, 24)), \"5th_percentile\": np.zeros((12, 24))}\n",
    "    month_hour_counts = {\"95th_percentile\": np.zeros((12, 24)), \"5th_percentile\": np.zeros((12, 24))}\n",
    "    \n",
    "    for val_index in range(len(val_dataset)):\n",
    "        doy, dow, target_data = get_doy_and_dow_from_validation(val_dataset, val_index)\n",
    "        seven_days_before = get_seven_days_before_target(target_data, X)\n",
    "        if seven_days_before is None: continue\n",
    "        \n",
    "        month = (doy // 30) % 12\n",
    "        if month == 0: month = 11 if doy > 330 else 0\n",
    "        \n",
    "        sample_cond, _ = val_dataset[val_index]\n",
    "        cond_flat = sample_cond.unsqueeze(0).to(device).view(1, -1)\n",
    "\n",
    "        samples_list = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_samples):\n",
    "                z = torch.randn(1, latent_dim).to(device)\n",
    "                mu_hat, logvar_hat = model.decoder(cond_flat, z)\n",
    "                eps = torch.randn_like(logvar_hat)\n",
    "                recon = mu_hat + eps * torch.exp(0.5 * logvar_hat)\n",
    "                samples_list.append(recon.cpu().numpy().flatten())\n",
    "\n",
    "        samples_array = np.array(samples_list)\n",
    "        percentile_95 = np.percentile(samples_array, 95, axis=0)\n",
    "        percentile_5 = np.percentile(samples_array, 5, axis=0)\n",
    "\n",
    "        mask_positive = target_data > 0\n",
    "        if np.any(mask_positive):\n",
    "            filtered_indices = np.where(mask_positive)[0]\n",
    "            filtered_target = target_data[mask_positive]\n",
    "            filtered_prediction = percentile_95[filtered_indices]\n",
    "            \n",
    "            for i, idx in enumerate(filtered_indices):\n",
    "                hour = idx % 24\n",
    "                error = abs(filtered_target[i] - filtered_prediction[i])\n",
    "                month_hour_errors[\"95th_percentile\"][month, hour] += error\n",
    "                month_hour_counts[\"95th_percentile\"][month, hour] += 1\n",
    "        \n",
    "        mask_negative = target_data < 0\n",
    "        if np.any(mask_negative):\n",
    "            filtered_indices = np.where(mask_negative)[0]\n",
    "            filtered_target = target_data[mask_negative]\n",
    "            filtered_prediction = percentile_5[filtered_indices]\n",
    "            \n",
    "            for i, idx in enumerate(filtered_indices):\n",
    "                hour = idx % 24\n",
    "                error = abs(filtered_target[i] - filtered_prediction[i])\n",
    "                month_hour_errors[\"5th_percentile\"][month, hour] += error\n",
    "                month_hour_counts[\"5th_percentile\"][month, hour] += 1\n",
    "    \n",
    "    season_mapping = {0: 0, 1: 0, 2: 1, 3: 1, 4: 1, 5: 2, 6: 2, 7: 2, 8: 3, 9: 3, 10: 3, 11: 0}\n",
    "    season_names = ['Winter', 'Spring', 'Summer', 'Fall']\n",
    "    \n",
    "    season_hour_errors = {\"95th_percentile\": np.zeros((4, 24)), \"5th_percentile\": np.zeros((4, 24))}\n",
    "    season_hour_counts = {\"95th_percentile\": np.zeros((4, 24)), \"5th_percentile\": np.zeros((4, 24))}\n",
    "    \n",
    "    for metric in [\"95th_percentile\", \"5th_percentile\"]:\n",
    "        for m in range(12):\n",
    "            season = season_mapping[m]\n",
    "            for h in range(24):\n",
    "                season_hour_errors[metric][season, h] += month_hour_errors[metric][m, h]\n",
    "                season_hour_counts[metric][season, h] += month_hour_counts[metric][m, h]\n",
    "    \n",
    "    avg_season_hour_errors = {\"95th_percentile\": np.zeros((4, 24)), \"5th_percentile\": np.zeros((4, 24))}\n",
    "    \n",
    "    for metric in [\"95th_percentile\", \"5th_percentile\"]:\n",
    "        for s in range(4):\n",
    "            for h in range(24):\n",
    "                if season_hour_counts[metric][s, h] > 0:\n",
    "                    avg_season_hour_errors[metric][s, h] = season_hour_errors[metric][s, h] / season_hour_counts[metric][s, h]\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(avg_season_hour_errors[\"95th_percentile\"], cmap=\"YlOrRd\", xticklabels=range(24), yticklabels=season_names, cbar_kws={'label': 'Mean Absolute Error'})\n",
    "    plt.title('95th Percentile - Season-Hour Error')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Season')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(avg_season_hour_errors[\"5th_percentile\"], cmap=\"YlOrRd\", xticklabels=range(24), yticklabels=season_names, cbar_kws={'label': 'Mean Absolute Error'})\n",
    "    plt.title('5th Percentile - Season-Hour Error')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Season')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\"season_hour_errors\": avg_season_hour_errors, \"season_hour_counts\": season_hour_counts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_seasonal_hourly_metrics(model, X, val_dataset, latent_dim, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_percentile_accuracy(model, X, val_dataset, latent_dim, num_samples=5000):\n",
    "    pos_percentiles = [97.5, 95, 92.5, 90, 87.5, 85, 82.5, 80, 77.5, 75]\n",
    "    neg_percentiles = [2.5, 5, 7.5, 10, 12.5, 15, 17.5, 20, 22.5, 25]\n",
    "    \n",
    "    pos_results = {p: [] for p in pos_percentiles}\n",
    "    neg_results = {p: [] for p in neg_percentiles}\n",
    "    \n",
    "    for val_index in range(len(val_dataset)):\n",
    "        _, _, target_data = get_doy_and_dow_from_validation(val_dataset, val_index)\n",
    "        if target_data is None or len(target_data) == 0: continue\n",
    "            \n",
    "        sample_cond, _ = val_dataset[val_index]\n",
    "        cond_flat = sample_cond.unsqueeze(0).to(device).view(1, -1)\n",
    "        \n",
    "        samples_list = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_samples):\n",
    "                z = torch.randn(1, latent_dim).to(device)\n",
    "                mu_hat, logvar_hat = model.decoder(cond_flat, z)\n",
    "                eps = torch.randn_like(logvar_hat)\n",
    "                recon = mu_hat + eps * torch.exp(0.5 * logvar_hat)\n",
    "                samples_list.append(recon.cpu().numpy().flatten())\n",
    "        \n",
    "        samples_array = np.array(samples_list)\n",
    "        mask_positive = target_data > 0\n",
    "        mask_negative = target_data < 0\n",
    "        \n",
    "        # Process positive percentiles\n",
    "        if np.any(mask_positive):\n",
    "            for p in pos_percentiles:\n",
    "                percentile_pred = np.percentile(samples_array, p, axis=0)\n",
    "                filtered_indices = np.where(mask_positive)[0]\n",
    "                filtered_target = target_data[mask_positive]\n",
    "                filtered_prediction = percentile_pred[filtered_indices]\n",
    "                \n",
    "                comparison_count = np.sum(filtered_target > filtered_prediction)\n",
    "                total_points = len(filtered_target)\n",
    "                percentage = (comparison_count / total_points * 100) if total_points > 0 else 0\n",
    "                pos_results[p].append(percentage)\n",
    "        \n",
    "        # Process negative percentiles\n",
    "        if np.any(mask_negative):\n",
    "            for p in neg_percentiles:\n",
    "                percentile_pred = np.percentile(samples_array, p, axis=0)\n",
    "                filtered_indices = np.where(mask_negative)[0]\n",
    "                filtered_target = target_data[mask_negative]\n",
    "                filtered_prediction = percentile_pred[filtered_indices]\n",
    "                \n",
    "                comparison_count = np.sum(filtered_target < filtered_prediction)\n",
    "                total_points = len(filtered_target)\n",
    "                percentage = (comparison_count / total_points * 100) if total_points > 0 else 0\n",
    "                neg_results[p].append(percentage)\n",
    "    \n",
    "    # Calculate averages\n",
    "    pos_avgs = [np.mean(pos_results[p]) if pos_results[p] else 0 for p in pos_percentiles]\n",
    "    neg_avgs = [np.mean(neg_results[p]) if neg_results[p] else 0 for p in neg_percentiles]\n",
    "    \n",
    "    # Create plots as line charts\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 8))\n",
    "    \n",
    "    # Positive values plot\n",
    "    ax1.plot(pos_avgs, pos_percentiles, 'bo-', markersize=8)\n",
    "    ax1.set_ylabel('Percentile')\n",
    "    ax1.set_xlabel('% cases where Target > Prediction')\n",
    "    ax1.set_title('Positive Values: Percentile vs Target Exceeding Prediction')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Add percentage values next to each point\n",
    "    for i, v in enumerate(pos_avgs):\n",
    "        ax1.text(v + 1, pos_percentiles[i], f'{v:.1f}%', va='center')\n",
    "    \n",
    "    # Negative values plot\n",
    "    ax2.plot(neg_avgs, neg_percentiles, 'ro-', markersize=8)\n",
    "    ax2.set_ylabel('Percentile')\n",
    "    ax2.set_xlabel('% cases where Target < Prediction')\n",
    "    ax2.set_title('Negative Values: Percentile vs Target Below Prediction')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Add percentage values next to each point\n",
    "    for i, v in enumerate(neg_avgs):\n",
    "        ax2.text(v + 1, neg_percentiles[i], f'{v:.1f}%', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'pos_percentiles': pos_percentiles,\n",
    "        'pos_percentages': pos_avgs,\n",
    "        'neg_percentiles': neg_percentiles,\n",
    "        'neg_percentages': neg_avgs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_percentile_accuracy(model, X, val_dataset, latent_dim, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_masking_impact(model, val_dataset, latent_dim, num_samples=5000):\n",
    "    hours_to_keep = list(range(0, 24))\n",
    "    positive_accuracy = {h: [] for h in hours_to_keep}\n",
    "    negative_accuracy = {h: [] for h in hours_to_keep}\n",
    "    positive_mae = {h: [] for h in hours_to_keep}\n",
    "    negative_mae = {h: [] for h in hours_to_keep}\n",
    "    \n",
    "    for val_index in range(len(val_dataset)):\n",
    "        _, _, target_data = get_doy_and_dow_from_validation(val_dataset, val_index)\n",
    "        if target_data is None: continue\n",
    "        \n",
    "        sample_cond, _ = val_dataset[val_index]\n",
    "        \n",
    "        for keep_hour in hours_to_keep:\n",
    "            masked_cond = sample_cond.clone()\n",
    "            for hour_idx in range(masked_cond.shape[1]):\n",
    "                if hour_idx > keep_hour:\n",
    "                    masked_cond[:, hour_idx] = 0.0\n",
    "            \n",
    "            cond_flat = masked_cond.unsqueeze(0).to(device).view(1, -1)\n",
    "            \n",
    "            samples_list = []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for _ in range(num_samples):\n",
    "                    z = torch.randn(1, latent_dim).to(device)\n",
    "                    mu_hat, logvar_hat = model.decoder(cond_flat, z)\n",
    "                    eps = torch.randn_like(logvar_hat)\n",
    "                    recon = mu_hat + eps * torch.exp(0.5 * logvar_hat)\n",
    "                    samples_list.append(recon.cpu().numpy().flatten())\n",
    "            \n",
    "            samples_array = np.array(samples_list)\n",
    "            percentile_95 = np.percentile(samples_array, 95, axis=0)\n",
    "            percentile_5 = np.percentile(samples_array, 5, axis=0)\n",
    "            \n",
    "            mask_positive = target_data > 0\n",
    "            mask_negative = target_data < 0\n",
    "            \n",
    "            if np.any(mask_positive):\n",
    "                filtered_indices = np.where(mask_positive)[0]\n",
    "                filtered_target = target_data[mask_positive]\n",
    "                filtered_p95 = percentile_95[filtered_indices]\n",
    "                \n",
    "                comparison_count = np.sum(filtered_target > filtered_p95)\n",
    "                total_points = len(filtered_target)\n",
    "                percentage = (comparison_count / total_points * 100) if total_points > 0 else 0\n",
    "                positive_accuracy[keep_hour].append(percentage)\n",
    "                \n",
    "                mae = np.mean(np.abs(filtered_target - filtered_p95))\n",
    "                positive_mae[keep_hour].append(mae)\n",
    "            \n",
    "            if np.any(mask_negative):\n",
    "                filtered_indices = np.where(mask_negative)[0]\n",
    "                filtered_target = target_data[mask_negative]\n",
    "                filtered_p5 = percentile_5[filtered_indices]\n",
    "                \n",
    "                comparison_count = np.sum(filtered_target < filtered_p5)\n",
    "                total_points = len(filtered_target)\n",
    "                percentage = (comparison_count / total_points * 100) if total_points > 0 else 0\n",
    "                negative_accuracy[keep_hour].append(percentage)\n",
    "                \n",
    "                mae = np.mean(np.abs(filtered_target - filtered_p5))\n",
    "                negative_mae[keep_hour].append(mae)\n",
    "    \n",
    "    pos_acc_avg = [np.mean(positive_accuracy[h]) if positive_accuracy[h] else np.nan for h in hours_to_keep]\n",
    "    neg_acc_avg = [np.mean(negative_accuracy[h]) if negative_accuracy[h] else np.nan for h in hours_to_keep]\n",
    "    pos_mae_avg = [np.mean(positive_mae[h]) if positive_mae[h] else np.nan for h in hours_to_keep]\n",
    "    neg_mae_avg = [np.mean(negative_mae[h]) if negative_mae[h] else np.nan for h in hours_to_keep]\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    ax1.plot(hours_to_keep, pos_acc_avg, 'bo-', markersize=6)\n",
    "    ax1.set_xlabel('Hours Kept (0 to N)')\n",
    "    ax1.set_ylabel('% Target > 95th Percentile')\n",
    "    ax1.set_title('Positive Values: PIC vs Hours Kept')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(hours_to_keep, neg_acc_avg, 'ro-', markersize=6)\n",
    "    ax2.set_xlabel('Hours Kept (0 to N)')\n",
    "    ax2.set_ylabel('% Target < 5th Percentile')\n",
    "    ax2.set_title('Negative Values: PIC vs Hours Kept')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    ax3.plot(hours_to_keep, pos_mae_avg, 'go-', markersize=6)\n",
    "    ax3.set_xlabel('Hours Kept (0 to N)')\n",
    "    ax3.set_ylabel('Mean Absolute Error')\n",
    "    ax3.set_title('Positive Values: DMAE vs Hours Kept')\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    ax4.plot(hours_to_keep, neg_mae_avg, 'mo-', markersize=6)\n",
    "    ax4.set_xlabel('Hours Kept (0 to N)')\n",
    "    ax4.set_ylabel('Mean Absolute Error')\n",
    "    ax4.set_title('Negative Values: DMAE vs Hours Kept')\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'hours_kept': hours_to_keep,\n",
    "        'positive_accuracy': pos_acc_avg,\n",
    "        'negative_accuracy': neg_acc_avg,\n",
    "        'positive_mae': pos_mae_avg,\n",
    "        'negative_mae': neg_mae_avg\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = plot_masking_impact(model, val_dataset, latent_dim, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_results = {\n",
    "    'hours_kept': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n",
    "    'positive_accuracy': [1.1386646577823045, 1.6814351226115931, 2.3274884157237095, 2.5913773046125987, 3.2486064589005763, 3.973077358371476, 4.42219702954997, 4.9360639904757555, 5.192258993729582, 5.615642364171776, 6.42528179881121, 6.188244761774174, 8.581650430179842, 8.777051597639833, 9.85650825209649, 11.001313446901683, 11.649064769653005, 13.346086810792695, 15.12754636431107, 16.501101240807124, 17.988813800578505, 19.015649002413706, 18.666358478123183, 19.43318424494895],\n",
    "    'negative_accuracy': [0.4331094171376182, 0.6096169637053298, 0.8251616199874694, 1.020303235628918, 1.2859422224388095, 1.521848638120506, 2.0077582862990715, 2.2432408599594664, 2.431123595472881, 2.792823855805286, 3.060923856051249, 3.2587396565424145, 3.8249142407922307, 4.190947716511253, 4.769613997906818, 5.009097450886272, 6.182088378701334, 6.0418294583920185, 6.995666637108428, 7.501198074619914, 7.6867976108860985, 8.25379795174564, 8.793456415195017, 9.654607837569463],\n",
    "    'positive_mae': [0.5871, 0.52436054, 0.47050986, 0.44112873, 0.40240493, 0.38478392, 0.34666067, 0.32964888, 0.31125548, 0.29373515, 0.2741771, 0.26104793, 0.2440422, 0.22862592, 0.21396622, 0.19872393, 0.18783732, 0.17526594, 0.16222188, 0.15280637, 0.14368306, 0.13874902, 0.13625775, 0.13246246],\n",
    "    'negative_mae': [0.7131765, 0.66213816, 0.62592065, 0.5922744, 0.56256324, 0.5346509, 0.49901715, 0.47765148, 0.4578438, 0.43206555, 0.4150181, 0.39353344, 0.37680808, 0.36417073, 0.34282902, 0.32877937, 0.31439, 0.30872813, 0.2921513, 0.2826057, 0.2795827, 0.26935375, 0.26004067, 0.24980125]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = plot_masking_impact(model, val_dataset, latent_dim, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(results1, results2, model1_name=\"Model 1\", model2_name=\"Model 2\"):\n",
    "    hours_to_keep = results1['hours_kept']\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Positive Accuracy\n",
    "    ax1.plot(hours_to_keep, results1['positive_accuracy'], 'bo-', markersize=6, label=model1_name)\n",
    "    ax1.plot(hours_to_keep, results2['positive_accuracy'], 'ro-', markersize=6, label=model2_name)\n",
    "    ax1.set_xlabel('Hours Kept (0 to N)')\n",
    "    ax1.set_ylabel('% Target > 95th Percentile')\n",
    "    ax1.set_title('Positive Values: PIC vs Hours Kept')\n",
    "    ax1.grid(True)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Negative Accuracy\n",
    "    ax2.plot(hours_to_keep, results1['negative_accuracy'], 'bo-', markersize=6, label=model1_name)\n",
    "    ax2.plot(hours_to_keep, results2['negative_accuracy'], 'ro-', markersize=6, label=model2_name)\n",
    "    ax2.set_xlabel('Hours Kept (0 to N)')\n",
    "    ax2.set_ylabel('% Target < 5th Percentile')\n",
    "    ax2.set_title('Negative Values: PIC vs Hours Kept')\n",
    "    ax2.grid(True)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Positive MAE\n",
    "    ax3.plot(hours_to_keep, results1['positive_mae'], 'go-', markersize=6, label=model1_name)\n",
    "    ax3.plot(hours_to_keep, results2['positive_mae'], 'mo-', markersize=6, label=model2_name)\n",
    "    ax3.set_xlabel('Hours Kept (0 to N)')\n",
    "    ax3.set_ylabel('Mean Absolute Error')\n",
    "    ax3.set_title('Positive Values: DMAE vs Hours Kept')\n",
    "    ax3.grid(True)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Negative MAE\n",
    "    ax4.plot(hours_to_keep, results1['negative_mae'], 'go-', markersize=6, label=model1_name)\n",
    "    ax4.plot(hours_to_keep, results2['negative_mae'], 'mo-', markersize=6, label=model2_name)\n",
    "    ax4.set_xlabel('Hours Kept (0 to N)')\n",
    "    ax4.set_ylabel('Mean Absolute Error')\n",
    "    ax4.set_title('Negative Values: DMAE vs Hours Kept')\n",
    "    ax4.grid(True)\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_comparison(model1, model2, \"Standard Model\", \"Enhanced Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
